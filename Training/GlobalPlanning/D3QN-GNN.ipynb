{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from sentence_transformers import util\n",
    "from nltk import word_tokenize\n",
    "from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import numpy as np\n",
    "from ConceptNet import ConceptNet\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from nltk import stem\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptnet = ConceptNet('Data/', numberbatch=False)\n",
    "glove_embedding = nn.Embedding.from_pretrained(conceptnet.concept_embedding.vectors).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.conv2 = GATConv(hidden_dim, output_dim)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        y = self.conv1(x, edge_index)\n",
    "        y = self.relu1(y)\n",
    "        y = self.conv2(y, edge_index)\n",
    "        y = self.relu2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class KeywordPredictor(nn.Module):\n",
    "    def __init__(self,\n",
    "                 linear_input_dim=1112,\n",
    "                 GCN_hidden_dim=512,\n",
    "                 GRU_hidden_dim=256,\n",
    "                 embedding_dim=300,\n",
    "                 cross_entropy_weight=[1],\n",
    "                 device='cuda:0',\n",
    "                 balanced_loss=False,\n",
    "                 GCN_layer='GAT',\n",
    "                 num_relations=-1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.input_dim = linear_input_dim\n",
    "        self.fc = nn.Sequential()\n",
    "        self.fc.add_module('linear1', nn.Linear(self.input_dim, 512))\n",
    "        self.fc.add_module('leaky_relu1', nn.ReLU())\n",
    "        self.fc.add_module('linear2', nn.Linear(512, 128))\n",
    "        self.fc.add_module('leaky_relu2', nn.ReLU())\n",
    "        self.fc.add_module('linear3', nn.Linear(128, 1))\n",
    "        self.fc = self.fc.to(self.device)\n",
    "\n",
    "        if not balanced_loss:\n",
    "            self.loss_func = nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor(cross_entropy_weight).to(device))\n",
    "        else:\n",
    "            self.loss_func = FocalLoss(ignore_index=-1)\n",
    "\n",
    "        self.GCN_input = embedding_dim\n",
    "        self.GCN_output = embedding_dim\n",
    "        self.GCN_hidden = GCN_hidden_dim\n",
    "        if GCN_layer == 'GCN':\n",
    "            self.GCN = GCN(self.GCN_input, self.GCN_hidden, self.GCN_output).to(device)\n",
    "        elif GCN_layer == 'GraphSAGE':\n",
    "            self.GCN = GraphSAGE(self.GCN_input, self.GCN_hidden, self.GCN_output).to(device)\n",
    "        elif GCN_layer == 'GAT':\n",
    "            self.GCN = GAT(self.GCN_input, self.GCN_hidden, self.GCN_output).to(device)\n",
    "        elif GCN_layer == 'TransEGCN':\n",
    "            self.relation_embedding = nn.Embedding(num_relations ,1113).cuda().requires_grad_(False)\n",
    "            self.GCN = KETransEGCN(self.GCN_input, self.GCN_hidden, self.GCN_output).to(device)\n",
    "        else:\n",
    "            self.GCN = None\n",
    "\n",
    "        self.GRU_input_dim = embedding_dim\n",
    "        self.GRU_hidden_dim = GRU_hidden_dim\n",
    "        self.GRU_layer_size = 1\n",
    "        self.GRU = nn.GRU(self.GRU_input_dim,\n",
    "                          self.GRU_hidden_dim,\n",
    "                          self.GRU_layer_size,\n",
    "                          bidirectional=True).to(self.device)\n",
    "\n",
    "        self.glove_embedding = nn.Embedding.from_pretrained(conceptnet.concept_embedding.vectors).to(device).requires_grad_(False)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc(state)\n",
    "        return x\n",
    "\n",
    "    def predict(self, state):\n",
    "        with torch.no_grad():\n",
    "            y = self.forward(state)\n",
    "        return y\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def candidate_nodes(graph, start_concepts, hops=2):\n",
    "    q = copy.deepcopy(start_concepts)\n",
    "    result = []\n",
    "    for i in range(hops):\n",
    "        temp = []\n",
    "        while len(q) != 0:\n",
    "            head = q.pop(0)\n",
    "            if not graph.has_node(head):\n",
    "                continue\n",
    "            if head not in result:\n",
    "                result.append(head)\n",
    "            adj = list(graph.neighbors(head))\n",
    "            temp.extend(adj)\n",
    "        q = list(set(temp))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch_geometric.utils import from_networkx\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QNet, self).__init__()\n",
    "\n",
    "        self.conv1 = GATConv(1112, 1).cuda()\n",
    "\n",
    "    def forward(self, x, egde_index):\n",
    "        x = self.conv1(x, egde_index)\n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AdvantageNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdvantageNet, self).__init__()\n",
    "\n",
    "        self.fc = nn.Sequential()\n",
    "        self.fc.add_module('linear1', nn.Linear(1112, 512))\n",
    "        self.fc.add_module('relu1', nn.LeakyReLU())\n",
    "        self.fc.add_module('linear2', nn.Linear(512, 128))\n",
    "        self.fc.add_module('relu2', nn.LeakyReLU())\n",
    "        self.fc.add_module('linear3', nn.Linear(128, 1))\n",
    "        self.fc = self.fc.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ContextEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 GRU_input_dim=300,\n",
    "                 GRU_hidden_dim=256):\n",
    "        super(ContextEncoder, self).__init__()\n",
    "\n",
    "        self.GRU_input_dim = GRU_input_dim\n",
    "        self.GRU_hidden_dim = GRU_hidden_dim\n",
    "        self.GRU_layer_size = 1\n",
    "        self.GRU = nn.GRU(\n",
    "            self.GRU_input_dim,\n",
    "            self.GRU_hidden_dim,\n",
    "            self.GRU_layer_size,\n",
    "            bidirectional=True\n",
    "        ).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _hn = self.GRU(x)\n",
    "        _hn = _hn.reshape(-1)\n",
    "        return _hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                GCN_input_dim,\n",
    "                GCN_hidden_dim,\n",
    "                GCN_output_dim\n",
    "                ):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "\n",
    "        # self.GCN_input = embedding_dim\n",
    "        self.GCN_input = GCN_input_dim\n",
    "        self.GCN_hidden = GCN_hidden_dim\n",
    "        self.GCN_output = GCN_output_dim\n",
    "        # self.GCN = GCN(self.GCN_input, self.GCN_hidden, 300).requires_grad_(True).to(device)\n",
    "        self.GCN = GAT(\n",
    "            self.GCN_input,\n",
    "            self.GCN_hidden,\n",
    "            self.GCN_output\n",
    "        ).cuda()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.GCN(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SharedLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 GCN_input_dim=300,\n",
    "                 GCN_hidden_dim=512,\n",
    "                 GCN_output_dim=300\n",
    "                 ):\n",
    "        super(SharedLayer, self).__init__()\n",
    "\n",
    "        self.graph_encoder = GraphEncoder(GCN_input_dim, GCN_hidden_dim, GCN_output_dim)\n",
    "        self.context_encoder = ContextEncoder()\n",
    "\n",
    "    def forward(self, global_graph, target_graph, context_ids):\n",
    "        _G_global = self.graph_encoder(glove_embedding(global_graph.x), global_graph.edge_index)\n",
    "        _G_target = self.graph_encoder(glove_embedding(target_graph.x), target_graph.edge_index)[0].repeat(global_graph.num_nodes, 1)\n",
    "        \n",
    "        _context_encoded = self.context_encoder(glove_embedding(context_ids))\n",
    "        _context_encoded = _context_encoded.reshape(-1).repeat(global_graph.num_nodes, 1)\n",
    "        # Graph Convolution\n",
    "        \n",
    "        x = torch.cat((_G_global, _G_target, _context_encoded), 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class D3QN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(D3QN, self).__init__()\n",
    "\n",
    "        self.shared_layer = SharedLayer()\n",
    "        self.q = QNet()\n",
    "        self.adv = AdvantageNet()\n",
    "\n",
    "    def forward(self, global_graph, target_graph, context_ids):\n",
    "        x = self.shared_layer(global_graph, target_graph, context_ids)\n",
    "        q, advantage = self.q(x, global_graph.edge_index), self.adv(x)\n",
    "        q_vals = q + (advantage - advantage.mean(dim=0, keepdim=True))\n",
    "        return q_vals\n",
    "    \n",
    "    def predict(self, global_graph, target_graph, context_ids):\n",
    "        with torch.no_grad():\n",
    "            q_vals = self.forward(global_graph, target_graph, context_ids)\n",
    "        return q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "\n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) >= self.buffer_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        sampled_batch = random.sample(self.buffer, batch_size)\n",
    "        return sampled_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulator:\n",
    "    def __init__(self, d3qn, glove_embedding, conceptnet, gamma=0.99, clip_ratio=0.2, device='cuda:0'):\n",
    "        self.labels = None\n",
    "        self.context_ids = None\n",
    "\n",
    "        self.G_global = None\n",
    "\n",
    "        self.episode_blacklist = []\n",
    "        self.blacklist_mask = []\n",
    "        self.context = []\n",
    "        self.user_bot = []\n",
    "        \n",
    "        self.chatbot_model_path = '../BlenderbotTraining/Chatbot/Model/'\n",
    "        self.chatbot_model_name = 'FullDatasetSuffixMultiK'\n",
    "        self.chatbot_model = BlenderbotForConditionalGeneration.from_pretrained(self.chatbot_model_path+'model/'+self.chatbot_model_name).cuda()\n",
    "        self.chatbot_tokenizer = BlenderbotTokenizer.from_pretrained(self.chatbot_model_path+'tokenizer/'+self.chatbot_model_name)\n",
    "        \n",
    "        model_name = 'facebook/blenderbot-400M-distill'\n",
    "        self.user_model = BlenderbotForConditionalGeneration.from_pretrained(model_name).cuda()\n",
    "        self.user_tokenizer = BlenderbotTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "        topic_model = f\"cardiffnlp/tweet-topic-21-multi\"\n",
    "        self.topic_tokenizer = AutoTokenizer.from_pretrained(topic_model)\n",
    "        self.topic_model = AutoModelForSequenceClassification.from_pretrained(topic_model).cuda()\n",
    "        self.topic_words = ['arts', 'business', 'celebrity', 'diaries', 'family', 'fashion', 'film', 'fitness', 'food', 'gaming', 'education', 'music', \n",
    "               'news', 'hobbies', 'relationships', 'science', 'sports', 'travel', 'student']\n",
    "        self.target_topic = None\n",
    "        self.topic_dist = None\n",
    "        \n",
    "        self.target_encoded = None\n",
    "\n",
    "        self.conceptnet = conceptnet\n",
    "        self.previous_dist = 10\n",
    "        self.target = None\n",
    "\n",
    "        self.global_graph = None\n",
    "        self.node_list = None\n",
    "        self.done = False\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.glove_embedding = glove_embedding\n",
    "        self.d3qn = d3qn\n",
    "        \n",
    "    def reset(self, initial_input, topic_idx):\n",
    "        global_target = self.topic_words[topic_idx]\n",
    "        self.target_topic = topic_idx\n",
    "        self.context = [initial_input]\n",
    "        self.previous_dist = 10\n",
    "        self.target = global_target\n",
    "        self.target_encoded = self.conceptnet.concept_embedding[global_target]\n",
    "\n",
    "        self.done = False\n",
    "        initial_concepts = concept_extractor(initial_input)\n",
    "\n",
    "        if len(initial_concepts) == 0:\n",
    "            initial_concepts = word_tokenize(initial_input)\n",
    "            tokenized_initial_concepts = []\n",
    "            for concept in initial_concepts:\n",
    "                if concept in self.conceptnet.concept_embedding.stoi and concept.isalpha():\n",
    "                    tokenized_initial_concepts.append(concept)\n",
    "            initial_concepts = tokenized_initial_concepts\n",
    "        \n",
    "        global_graph = self.conceptnet.bidirectional_reasoning(initial_concepts, global_target)\n",
    "        for start_concept in initial_concepts:\n",
    "            global_graph = nx.compose(global_graph, self.conceptnet.bidirectional_reasoning([global_target], start_concept, K=10, hops=3))\n",
    "\n",
    "        \n",
    "        self.global_graph = global_graph\n",
    "        self.G_global = from_networkx(self.global_graph).cuda()\n",
    "        self.node_list = list(global_graph.nodes)\n",
    "        self.conceptnet_subgraph = self.conceptnet.conceptnet.subgraph(self.node_list)\n",
    "            \n",
    "        target_neighbours = list(global_graph.neighbors(self.target))\n",
    "        target_graph = nx.Graph()\n",
    "        target_graph.add_node(self.target, x=conceptnet.concept_embedding.stoi[self.target])\n",
    "        for target_neighbour in target_neighbours:\n",
    "            target_graph.add_node(target_neighbour, x=conceptnet.concept_embedding.stoi[target_neighbour])\n",
    "            target_graph.add_edge(self.target, target_neighbour)\n",
    "        self.target_graph = target_graph   \n",
    "        self.G_target = from_networkx(self.target_graph).cuda()\n",
    "        \n",
    "        self.blacklist_mask = []\n",
    "        self.episode_blacklist = []\n",
    "        for node in self.node_list:\n",
    "            if node in blacklist:\n",
    "                self.blacklist_mask.append(0)\n",
    "                self.episode_blacklist.append(node)\n",
    "            else:\n",
    "                self.blacklist_mask.append(1)\n",
    "        self.blacklist_mask = torch.tensor(self.blacklist_mask)\n",
    "\n",
    "        g, t, c, m = self.get_state()\n",
    "\n",
    "        return (g, t, c, m)\n",
    "\n",
    "    def generate_response(self, action):\n",
    "        keywords = [self.node_list[a] for a in action]\n",
    "        print(\"[Keywords] \", keywords)\n",
    "        \n",
    "        concat_tokens =  '<s>' + '</s><s>'.join(self.context[-3:]) + '<keyword> ' + '</keyword><keyword>'.join(keywords)+'</keyword>'\n",
    "        input_tokenized = self.chatbot_tokenizer(concat_tokens, padding='max_length', truncation=True,\n",
    "                                      max_length=128, return_tensors='pt')\n",
    "        # Generate a response\n",
    "        response = self.chatbot_model.generate(input_ids=input_tokenized.input_ids.cuda(),\n",
    "                                              attention_mask=input_tokenized.attention_mask.cuda())\n",
    "        # Decode and print the response\n",
    "        response_text = self.chatbot_tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "        return response_text\n",
    "    \n",
    "    \n",
    "    def greedy_action(self, state):\n",
    "        action_masked = (state[3] == 0).reshape(-1).float()\n",
    "        action_black_list_mixed = action_masked * self.blacklist_mask.cuda()\n",
    "        if action_black_list_mixed[action_black_list_mixed == 1].size(0) != 0:\n",
    "            action_masked = action_black_list_mixed\n",
    "        valid_elems = [(idx, elem) for idx, elem in enumerate(self.node_list) if action_masked[idx] == 1]\n",
    "        \n",
    "        l = 10\n",
    "        greedy_action = None\n",
    "        for idx, elem in valid_elems:\n",
    "            _l = nx.shortest_path_length(self.global_graph, elem, self.target)\n",
    "            if _l < l:\n",
    "                l = _l\n",
    "                greedy_action = idx\n",
    "        return greedy_action\n",
    "                \n",
    "    \n",
    "    def step(self, action):\n",
    "        # generate chatbot response and add response to context list\n",
    "        sentence = self.generate_response(action)\n",
    "        self.context.append(sentence)\n",
    "        concat_context = ' <s> ' + ' </s> <s> '.join(self.context[-3:]) + ' </s> '\n",
    "\n",
    "        input_tokenized = self.user_tokenizer(concat_context, padding='max_length', truncation=True,\n",
    "                                      max_length=128, return_tensors='pt')\n",
    "        \n",
    "        user_response_tokens = self.user_model.generate(input_ids=input_tokenized.input_ids.cuda(),\n",
    "                                                          attention_mask=input_tokenized.attention_mask.cuda())\n",
    "        user_response_text = self.user_tokenizer.decode(user_response_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "        user_concepts = word_tokenize(user_response_text)\n",
    "\n",
    "        self.context.append(user_response_text)\n",
    "        reward = self.reward(user_response_text)\n",
    "        \n",
    "        done = self.is_complete(user_concepts)\n",
    "        self.done = done\n",
    "        \n",
    "\n",
    "        g, t, c, m = self.get_state()\n",
    "\n",
    "        return (g, t, c, m), reward, done\n",
    "\n",
    "    def get_state(self):\n",
    "        bridge_concepts = list(set(word_tokenize(self.context[-1])) & set(list(self.global_graph.nodes)))\n",
    "\n",
    "        if len(bridge_concepts) == 0:\n",
    "            bridge_concepts = word_tokenize(self.context[-2]) if len(self.context) >= 2 else word_tokenize(self.context[0])\n",
    "            tokenized_concepts = []\n",
    "            for concept in bridge_concepts:\n",
    "                if concept in self.conceptnet.concept_embedding.stoi and concept.isalpha():\n",
    "                    tokenized_concepts.append(concept)\n",
    "            bridge_concepts = list(set(tokenized_concepts) & set(list(self.global_graph.nodes)))\n",
    "\n",
    "        candidates = set(candidate_nodes(self.global_graph, bridge_concepts, 2))\n",
    "        global_graph_nodes = self.global_graph.nodes\n",
    "        global_node_mapping = dict(zip(list(global_graph_nodes), range(len(global_graph_nodes))))\n",
    "        candidate_indices = np.array([global_node_mapping[node] for node in candidates])\n",
    "        \n",
    "        candidate_mask = torch.ones(len(global_graph_nodes), dtype=int) * 0\n",
    "        candidate_mask[candidate_indices.tolist()] = 1\n",
    "        \n",
    "        edge_index = self.G_global.edge_index\n",
    "        G_global_x = self.G_global.x\n",
    "\n",
    "        context_tokens = []\n",
    "        for context in self.context:\n",
    "            context_tokens.extend(word_tokenize(context))\n",
    "        context_ids = []\n",
    "        for concept in context_tokens:\n",
    "            if concept in self.conceptnet.concept_embedding.stoi:\n",
    "                # context_ids.append(self.conceptnet.concept_embedding[concept])\n",
    "                context_ids.append(self.conceptnet.concept_embedding.stoi[concept])\n",
    "        context_x = torch.tensor(context_ids).cuda()\n",
    "        \n",
    "        return self.G_global, self.G_target, context_x, candidate_mask\n",
    "\n",
    "    def get_rl_state(self, G_global, G_target, context_x):\n",
    "        # get global graph GLoVe\n",
    "        G_graph_embd = self.glove_embedding(G_global_x).cuda()\n",
    "        # get target GLoVe\n",
    "        G_target_embd = self.glove_embedding(G_target)[0].repeat(G_global_x.size(0), 1).cuda()\n",
    "        # get context embedding\n",
    "        context_ids = self.glove_embedding(context_x).cuda()\n",
    "        return G_graph_embd, G_target_embd, context_ids\n",
    "\n",
    "    def get_keyword(self, text):\n",
    "        concepts = [concept for concept in word_tokenize(text)\n",
    "                    if (concept in list(self.global_graph.nodes))]\n",
    "        concepts = list(set(concepts))\n",
    "        return concepts\n",
    "\n",
    "    def reward(self, user_text):\n",
    "        w = 0\n",
    "        r = 0\n",
    "                        \n",
    "        topic_tokens = self.topic_tokenizer(user_text, return_tensors='pt')\n",
    "        topic_output = self.topic_model(input_ids=topic_tokens.input_ids.cuda(), \n",
    "                                        attention_mask=topic_tokens.attention_mask.cuda())\n",
    "\n",
    "        topic_scores = topic_output[0][0].cpu().detach().numpy()\n",
    "        topic_scores = expit(topic_scores)\n",
    "        \n",
    "        self.topic_dist = topic_scores[self.target_topic]\n",
    "        r += self.topic_dist\n",
    "        \n",
    "        if self.is_complete([]):\n",
    "            r += 10\n",
    "\n",
    "        return r\n",
    "\n",
    "    def is_complete(self, user_concepts):\n",
    "        if self.topic_dist > 0.5:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lr = 1e-6\n",
    "max_timesteps = 30\n",
    "gamma=0.99\n",
    "epsilon_clip=0.2\n",
    "value_coef = 0.5\n",
    "entropy_coef = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_to_remove = [(u, v) for u, v in conceptnet.conceptnet.edges() if u == v]  # Find self-loops\n",
    "\n",
    "for edge in edges_to_remove:\n",
    "    conceptnet.conceptnet.remove_edge(*edge) \n",
    "    \n",
    "blacklist_nodes = list(set(conceptnet.conceptnet.nodes) & set(blacklist))\n",
    "conceptnet.conceptnet.remove_nodes_from(blacklist_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D3QN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_gpu_usage():\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'], stdout=subprocess.PIPE)\n",
    "        gpu_memory_used = int(result.stdout.decode('utf-8').strip())\n",
    "        return gpu_memory_used\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching GPU usage:\", str(e))\n",
    "        return None\n",
    "\n",
    "gpu_usage = get_gpu_usage()\n",
    "if gpu_usage is not None:\n",
    "    print(f\"GPU Memory Used: {gpu_usage} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon = 0.9\n",
    "target_update_freq = 5\n",
    "max_episode = 20\n",
    "buffer_size = 10000\n",
    "hidden_size = 64\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_path = 'model/cross_entropy_batch_32_lr_1e-05_lr_decay_8e-01_bdr_class_balanced_1_GAT_original_2_hop/model_epoch666.pth'\n",
    "state_dict = torch.load(load_model_path)\n",
    "keyword_predictor = KeywordPredictor()\n",
    "keyword_predictor.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "policy_net = D3QN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net = D3QN()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net.shared_layer.graph_encoder.GCN.load_state_dict(keyword_predictor.GCN.state_dict())\n",
    "policy_net.shared_layer.context_encoder.GRU.load_state_dict(keyword_predictor.GRU.state_dict())\n",
    "policy_net.adv.fc.load_state_dict(keyword_predictor.fc.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_predictor = None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Simulator(policy_net, glove_embedding, conceptnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_usage = get_gpu_usage()\n",
    "if gpu_usage is not None:\n",
    "    print(f\"GPU Memory Used: {gpu_usage} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'starting.txt'\n",
    "\n",
    "f = open(path)\n",
    "lines = f.readlines()\n",
    "\n",
    "targets = []\n",
    "startings = []\n",
    "\n",
    "for line in lines:\n",
    "    startings.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import time\n",
    "\n",
    "\n",
    "writer = SummaryWriter('./logs/{}'.format('OriginalModelTopicRemoveBlacklist'))\n",
    "pbar = tqdm(range(num_epochs))\n",
    "updated_epoch = 0\n",
    "global_step = 0\n",
    "mse_loss = nn.MSELoss()\n",
    "grad_scaler = GradScaler()\n",
    "topic_idx = 0\n",
    "\n",
    "for epoch in pbar:\n",
    "    topic_idx = (topic_idx + 1) % 19\n",
    "    staring_idx = random.randint(0, len(startings)-1)\n",
    "    \n",
    "    print(\"[Target] \", env.topic_words[topic_idx])\n",
    "    \n",
    "    state = env.reset(startings[staring_idx], topic_idx)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    episode = 0\n",
    "    \n",
    "    while not done and episode < max_episode:\n",
    "        print(episode)\n",
    "        global_step += 1\n",
    "        \n",
    "        epsilon = max(epsilon * 0.99, 0.1)  # Decay exploration epsilon\n",
    "        if np.random.random() < epsilon:\n",
    "            valid_actions = [idx for idx, elem in enumerate(env.node_list) if state[3][idx] == 1]\n",
    "            action = random.sample(valid_actions, k=min(1, len(valid_actions)))\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = policy_net.predict(state[0], state[1], state[2]).reshape(-1)\n",
    "                q_values[state[3] == 0] = -50\n",
    "                _, action = q_values.topk(min(1, state[3][state[3] == 1].size(0)))\n",
    "                \n",
    "                writer.add_scalar('Q_value', q_values[state[3] == 1].max().item(), global_step)\n",
    "        print(action)\n",
    "\n",
    "        next_state, reward, done = env.step(action)\n",
    "        replay_buffer.add((state, action, reward, next_state, done))\n",
    "        total_reward += reward\n",
    "\n",
    "        if len(replay_buffer.buffer) >= batch_size:\n",
    "            \n",
    "            policy_net.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            batch = replay_buffer.sample(batch_size)\n",
    "            \n",
    "            loss = 0\n",
    "            with autocast():\n",
    "                q_value_batch = []\n",
    "                next_q_value_batch = []\n",
    "                for (_state, _action, _reward, _next_state, _done) in batch:\n",
    "                    q_values = policy_net(_state[0], _state[1], _state[2]).reshape(-1)\n",
    "                    q_values = q_values[_action]\n",
    "                    q_value_batch.append(q_values)\n",
    "\n",
    "                    next_q_values = target_net(_next_state[0], _next_state[1], _next_state[2]).reshape(-1).detach().clone().requires_grad_(False)\n",
    "                    next_q_policy_values = policy_net(_next_state[0], _next_state[1], _next_state[2]).reshape(-1)\n",
    "                    \n",
    "                    next_q_policy_values[(_next_state[3] == 0).reshape(-1)] = -50\n",
    "                    next_q_values_indices = next_q_policy_values.topk(len(_action)).indices\n",
    "\n",
    "                    expected_q_values = torch.tensor([_reward]).float().clone().detach().cuda() + gamma * (1 - _done) * next_q_values[next_q_values_indices]\n",
    "                    next_q_value_batch.append(expected_q_values.squeeze())\n",
    "\n",
    "                loss = mse_loss(torch.stack(q_value_batch), \n",
    "                                torch.stack(next_q_value_batch))\n",
    "                print(loss)\n",
    "\n",
    "                writer.add_scalar('policy/loss', loss.item(), updated_epoch)\n",
    "                writer.add_scalar('buffer', len(replay_buffer.buffer), updated_epoch)\n",
    "                gpu_usage = get_gpu_usage()\n",
    "                writer.add_scalar('GPU', gpu_usage, updated_epoch)\n",
    "                updated_epoch += 1\n",
    "\n",
    "                grad_scaler.scale(loss).backward()\n",
    "                grad_scaler.step(optimizer)\n",
    "                grad_scaler.update()\n",
    "            \n",
    "            gpu_usage = get_gpu_usage()\n",
    "\n",
    "            # Update target network\n",
    "            if epoch % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        state = next_state\n",
    "        episode += 1\n",
    "\n",
    "    pbar.set_description(f\"Epoch {epoch+1}, Total Reward: {total_reward}, Done: {done}, Buffer: {len(replay_buffer.buffer)}\")\n",
    "    writer.add_scalar('reward', total_reward, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), './model/D3GQN/D3GQN_topic_glove.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(replay_buffer, open('replayBuffer.txt', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net_state_dict = torch.load('./model/D3GQN/D3GQN_topic_cycle.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net.load_state_dict(policy_net_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "valid_steps_list = []\n",
    "achieve_rate_list = []\n",
    "\n",
    "f = open('eval_log.txt', 'w')\n",
    "\n",
    "policy_net.eval()\n",
    "for t in range(14, 19):\n",
    "    test_epochs = 50\n",
    "    pbar = tqdm(range(test_epochs))\n",
    "    topic_idx = t\n",
    "    print(\"[Target] \", env.topic_words[topic_idx])\n",
    "    f.write(\"[Target] \" + env.topic_words[topic_idx]+'\\n')\n",
    "\n",
    "    task_completion = []\n",
    "    steps = []\n",
    "    starting_sampled = random.sample(startings, 50)\n",
    "\n",
    "    for epoch in pbar:\n",
    "        staring_idx = random.randint(0, len(startings)-1)\n",
    "        print('[Starting]', starting_sampled[epoch])\n",
    "\n",
    "        state = env.reset(starting_sampled[epoch], topic_idx)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        episode = 0\n",
    "\n",
    "        while not done and episode < max_episode:\n",
    "            with torch.no_grad():\n",
    "                q_values = policy_net.predict(state[0], state[1], state[2]).reshape(-1)\n",
    "                q_values[state[3] == 0] = -50\n",
    "                _, action = q_values.topk(min(3, state[3][state[3] == 1].size(0)))\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            state = next_state\n",
    "            episode += 1\n",
    "\n",
    "        task_completion.append(done)\n",
    "        steps.append(episode)\n",
    "        \n",
    "        print(starting_sampled[epoch])\n",
    "        print(f\"Epoch {epoch+1}, Total Reward: {total_reward}, Done: {done}, Steps: {episode}\")\n",
    "        f.write(\"[Starting]\"+starting_sampled[epoch]+'\\n')\n",
    "        f.write(f\"Epoch {epoch+1}, Total Reward: {total_reward}, Done: {done}, Steps: {episode}\"+'\\n')\n",
    "    valid_steps = [step for idx, step in enumerate(steps) if task_completion[idx]]\n",
    "    mean_achieve_steps = sum(valid_steps) / len(valid_steps)\n",
    "    achieve_rate = sum(task_completion) / len(task_completion)\n",
    "    print(mean_achieve_steps, achieve_rate)\n",
    "    \n",
    "    valid_steps_list.append(mean_achieve_steps)\n",
    "    achieve_rate_list.append(achieve_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_steps_list, achieve_rate_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'facebook/blenderbot-3B'\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(model_name).cuda()\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = f\"cardiffnlp/tweet-topic-21-multi\"\n",
    "topic_tokenizer = AutoTokenizer.from_pretrained(topic_model)\n",
    "topic_model = AutoModelForSequenceClassification.from_pretrained(topic_model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "topic_list = []\n",
    "for starting in tqdm(random.sample(startings, 50)):\n",
    "    episode = 0\n",
    "    \n",
    "    context = [starting]\n",
    "    topics = []\n",
    "    print(starting)\n",
    "    while episode < 20:\n",
    "        episode += 1\n",
    "        concat_context = ' <s> ' + ' </s> <s> '.join(context[-3:]) + ' </s> '\n",
    "        input_tokenized = tokenizer(concat_context, padding='max_length', truncation=True,\n",
    "                                      max_length=128, return_tensors='pt')\n",
    "        chatbot_response_tokens = model.generate(input_ids=input_tokenized.input_ids.cuda(),\n",
    "                                                          attention_mask=input_tokenized.attention_mask.cuda())\n",
    "        chatbot_response_text = tokenizer.decode(chatbot_response_tokens[0], skip_special_tokens=True)\n",
    "        context.append(chatbot_response_text)\n",
    "        \n",
    "        concat_context = ' <s> ' + ' </s> <s> '.join(context[-3:]) + ' </s> '\n",
    "        input_tokenized = tokenizer(concat_context, padding='max_length', truncation=True,\n",
    "                                      max_length=128, return_tensors='pt')\n",
    "        user_response_tokens = model.generate(input_ids=input_tokenized.input_ids.cuda(),\n",
    "                                                          attention_mask=input_tokenized.attention_mask.cuda())\n",
    "        user_response_text = tokenizer.decode(user_response_tokens[0], skip_special_tokens=True)\n",
    "        \n",
    "        context.append(user_response_text)\n",
    "        \n",
    "    \n",
    "        topic_tokens = topic_tokenizer(user_response_text, return_tensors='pt')\n",
    "        topic_output = topic_model(input_ids=topic_tokens.input_ids.cuda(), \n",
    "                                        attention_mask=topic_tokens.attention_mask.cuda())\n",
    "\n",
    "        topic_scores = topic_output[0][0].cpu().detach().numpy()\n",
    "        topic_scores = expit(topic_scores)\n",
    "        \n",
    "        print(\"[Chatbot]\", chatbot_response_text)\n",
    "        print(\"[Userbot]\", user_response_text)\n",
    "        \n",
    "        topics.append(topic_scores)\n",
    "    topic_list.append(topics)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "topic_words = np.array(['arts', 'business', 'celebrity', 'diaries', 'family', 'fashion', 'film', 'fitness', 'food', 'gaming', 'education', 'music', \n",
    "               'news', 'hobbies', 'relationships', 'science', 'sports', 'travel', 'student'])\n",
    "\n",
    "print(len(topic_list), len(topic_list[0]), topic_list[0][0].shape)\n",
    "\n",
    "\n",
    "topic_achivement = {}\n",
    "for k in range(len(topic_list)):\n",
    "    for t in topic_list[k]:\n",
    "        tws = topic_words[t > 0.5]\n",
    "        for tw in tws:\n",
    "            if tw not in topic_achivement:\n",
    "                topic_achivement[tw] = 0\n",
    "            topic_achivement[tw] += 1\n",
    "\n",
    "[(k, topic_achivement[k]/1107) for k in topic_achivement]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
